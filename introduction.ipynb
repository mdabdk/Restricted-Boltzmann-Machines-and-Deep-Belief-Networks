{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines and Deep Belief Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Restricted Boltzmann Machine (RBM) is a probabilistic graphical model that consists of visible random variables (or units) $\\mathbf{v}$ and hidden random variables (or units) $\\mathbf{h}$, where:\n",
    "\n",
    "$$\\mathbf{v}=\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_N \\end{bmatrix}$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\mathbf{h}=\\begin{bmatrix} h_1 \\\\ h_2 \\\\ \\vdots \\\\ h_M \\end{bmatrix}$$\n",
    "\n",
    "See [section 16.5](https://www.deeplearningbook.org/contents/graphical_models.html) in the book titled \"Deep Learning\" by Ian Goodfellow et al. for why the hidden vector $\\mathbf{h}$ is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution represented by all [Boltzmann Machines](https://en.wikipedia.org/wiki/Boltzmann_machine) (not just an RBM) is:\n",
    "\n",
    "$$p(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$\n",
    "\n",
    "Where $\\mathbf{W}$ is a parameter matrix, $\\mathbf{a}$ and $\\mathbf{b}$ are parameter vectors, and:\n",
    "\n",
    "$$E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = -\\mathbf{a}^T\\mathbf{v} - \\mathbf{b}^T\\mathbf{h} - \\mathbf{v}^T\\mathbf{W}\\mathbf{h}$$\n",
    "\n",
    "$$Z = \\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E$ is know as the **energy function** and $Z$ is known as the **partition function**. However, to make parameter estimation (learning) and inference tractable, a **Restricted** Boltzmann Machine introduces the following assumptions:\n",
    "\n",
    "$$ p(\\mathbf{v}|\\mathbf{h}) = \\prod_{i=1}^N p(v_i|\\mathbf{h})$$\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\prod_{j=1}^M p(h_i|\\mathbf{v})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, it is assumed that the individual visible random variables (units) are conditionally independent given all the hidden units $\\mathbf{h}$. Also, it is assumed that the individual hidden units are conditionally independent given all the visible units $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable to estimate the marginal probability distribution $p(\\mathbf{v})$ of all the visible units $\\mathbf{v}$, as this reveals relationships between the visible units. This marginal probability distribution can be obtained by marginalizing the joint distribution $p(\\mathbf{v},\\mathbf{h})$ over all the hidden units $\\mathbf{h}$:\n",
    "\n",
    "$$p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marginal probability distribution can therefore be estimated using maximum likelihood estimation. Assuming that the visible units $\\mathbf{v}$ have been observed, then the equation above also represents the likelihood function. The log-likelihood function is therefore:\n",
    "\n",
    "$$ln(p(\\mathbf{v};\\mathbf{\\theta})) = ln\\left(\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right) - ln\\left(\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\theta = \\begin{bmatrix} \\mathbf{W} \\\\ \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix}$$\n",
    "\n",
    "For brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the log-likelihood function is not in closed form, which means that it is difficult to compute analytically, then gradient ascent will be used to maximize it. The gradient of the log-likelihood function is:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\frac{1}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\frac{p(\\mathbf{v},\\mathbf{h})}{p(\\mathbf{v})} = \\frac{\\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since:\n",
    "\n",
    "$$\\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{Z} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = p(\\mathbf{v},\\mathbf{h})$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{v},\\mathbf{h})\\right] \\\\\n",
    "&= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both of these are expectations, they can be approximated using [Monte Carlo integration](http://people.duke.edu/~ccc14/sta-663-2019/notebook/S14D_Monte_Carlo_Integration.html#Intuition-behind-Monte-Carlo-integration):\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} \\approx -\\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] +\n",
    "\\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term can be computed because it is easy to sample from $p(\\mathbf{h}|\\mathbf{v})$. However, it is difficult to sample from $p(\\mathbf{v},\\mathbf{h})$ directly, but since it is easy to sample from $p(\\mathbf{v}|\\mathbf{h})$, then Gibbs sampling is used to sample from both $p(\\mathbf{h}|\\mathbf{v})$ and $p(\\mathbf{v}|\\mathbf{h})$ to approximate a sample from $p(\\mathbf{v},\\mathbf{h})$. Also, notice that:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx \\frac{\\partial}{\\partial \\mathbf{\\theta}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta}) \\right] \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this means that the gradient of the log-likelihood function can be approximated by the gradient of the difference of two sample means. Once the gradient of the log-likelihood function is estimated for a particular set of observed visible units $\\mathbf{v}$, then the parameters $\\mathbf{W},\\mathbf{a},$ and $\\mathbf{b}$ can be updated using the following gradient ascent update rules:\n",
    "\n",
    "$$\\mathbf{W}_{t+1} = \\mathbf{W}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "$$\\mathbf{a}_{t+1} = \\mathbf{a}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}}$$\n",
    "\n",
    "$$\\mathbf{b}_{t+1} = \\mathbf{b}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}}$$\n",
    "\n",
    "Where $\\epsilon$ is the learning rate and:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}} \\approx \\frac{\\partial}{\\partial \\mathbf{W}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}} \\approx \\frac{\\partial}{\\partial \\mathbf{a}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}} \\approx \\frac{\\partial}{\\partial \\mathbf{b}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of an RBM is inspired by the paper titled [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) by Ruslan Salakhutdinov et al., which will from now on be referenced as [1].\n",
    "\n",
    "More precisely, one of the RBM architectures that the authors implemented involved categorical (or softmax) visible units and Bernoulli (or binary) hidden units. Also, in the paper, the authors use data from the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) competition that consists of approximately 100 million user ratings. 480,000 users gave approximately 18,000 movies a rating from 1 to 5. Therefore, the main objective of this implementation is to use these user ratings as the categorical visible units with Bernoulli hidden units and estimate the corresponding parameters using maximum likelihood estimation. However, to keep the implementation simple, only 50 movies per user will be considered.\n",
    "\n",
    "This RBM will be implemented using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering 50 movies per user, to make the categorical-Bernoulli RBM easier to understand, suppose instead that there are only 4 movie ratings per user and that the relationships between these movie ratings will be encoded in only 3 hidden Bernoulli units. Note that since user ratings range from 1 to 5, they will need to be expressed in one-hot encoding format (see [here](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that given that there are 4 movie ratings per user ranging from 1 to 5, then the visible units will be expressed in a $4 \\times 5$ matrix, denoted $\\mathbf{V}$, where the $i^{th}$ row corresponds to the $i^{th}$ movie per user, and the $j^{th}$ column of the $i^{th}$ row corresponds to whether that user gave a rating of $j$ to the $i^{th}$ movie. For example, $\\mathbf{V}$ could be:\n",
    "\n",
    "$$\n",
    "\\mathbf{V} =\n",
    "\\begin{bmatrix} \n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that this user gave the first movie a rating of 2, the second movie a rating of 3, the third movie a rating of 1, and the fourth movie a rating of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, given $M$ movie ratings per user, with each rating ranging from 1 to $N$, then $\\mathbf{V}$ is a $M \\times N$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{V} = \n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} & \\dots & v_{1N} \\\\\n",
    "v_{21} & v_{22} & v_{23} & \\dots & v_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{M1} & v_{M2} & v_{M3} & \\dots & v_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, one example of a $\\mathbf{V}$ matrix is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_visible = 4\n",
    "num_categories = 5\n",
    "V = torch.randn(num_visible,num_categories)\n",
    "# make sure each row is a categorical distribution\n",
    "V = torch.nn.functional.softmax(V,dim = 1)\n",
    "# sample\n",
    "V = torch.distributions.one_hot_categorical.OneHotCategorical(probs = V).sample().to(torch.int)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the hidden units are Bernoulli (binary), then it is sufficient to model the hidden units as a $K \\times 1$ column vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "\\vdots \\\\\n",
    "h_K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For example, in the case of 3 hidden units, $\\mathbf{h}$ could be:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, one example of $\\mathbf{h}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden = 3\n",
    "h = torch.randn(num_hidden)\n",
    "# make sure each element is a Bernoulli distribution\n",
    "h = torch.sigmoid(h)\n",
    "# sample\n",
    "h = torch.distributions.bernoulli.Bernoulli(probs = h).sample().to(torch.int)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to equation (1) in section 2.1 from [1], the conditional probability that the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$ is 1 given that the hidden vector $\\mathbf{h}$ is observed is modelled as a logistic model:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\mathbf{w}_{ij}^T \\mathbf{h} + b_{ij})}{\\sum_j \\exp(\\mathbf{w}_{ij}^T \\mathbf{h} + b_{ij})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{w}_{ij}$ is the weight vector associated with the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$.\n",
    "* $b_{ij}$ is the scalar bias associated with the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$.\n",
    "\n",
    "In other words, each row of $\\mathbf{V}$ is sampled from a categorical distribution then converted to one-hot encoding.\n",
    "\n",
    "Note that since an inner product is equivalent to an element-wise multiplication followed by a summation, and if:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{ij} =\n",
    "\\begin{bmatrix}\n",
    "w_{ij}^1 \\\\\n",
    "w_{ij}^2 \\\\\n",
    "w_{ij}^3 \\\\\n",
    "\\vdots \\\\\n",
    "w_{ij}^K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the above conditional probability distribution can also be expressed as:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}{\\sum_j \\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}\n",
    "$$\n",
    "\n",
    "Which is how this conditional probability distribution is expressed in equation (1) in section 2.1 in [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each element of $\\mathbf{V}$ is associated with a weight vector $\\mathbf{w}_{ij}$ and bias $b_{ij}$, then it is helpful to express these weights and biases as matrices. The weight vectors $\\mathbf{w}_{ij}$ can be stored in a 3-dimensional matrix, denoted $\\mathbf{W}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}_{11} & \\mathbf{w}_{12} & \\mathbf{w}_{13} & \\dots & \\mathbf{w}_{1N} \\\\\n",
    "\\mathbf{w}_{21} & \\mathbf{w}_{22} & \\mathbf{w}_{23} & \\dots & \\mathbf{w}_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{w}_{M1} & \\mathbf{w}_{M2} & \\mathbf{w}_{M3} & \\dots & \\mathbf{w}_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that $\\mathbf{W}$ has the dimensions $M \\times N \\times K$, where $K$ is the dimensionality of the hidden vector $\\mathbf{h}$. In the case of 4 movie ratings per user, 5 possible ratings, and 3 hidden units, then $\\mathbf{W}$ has a size of $4 \\times 5 \\times 3$, and each element is a vector of size $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the scalar biases can be stored in a 2-dimensional matrix, denoted $\\mathbf{B}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & b_{13} & \\dots & b_{1N} \\\\\n",
    "b_{21} & b_{22} & b_{23} & \\dots & b_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{M1} & b_{M2} & b_{M3} & \\dots & b_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the case of 4 movie ratings per user and 5 possible ratings, then $\\mathbf{B}$ has a size of $4 \\times 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\sum_p [\\mathbf{W}\\mathbf{h}] + \\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "\\sum_p [w_{11}^p h_p] + b_{11} & \\sum_p [w_{12}^p h_p] + b_{12} & \\dots & \\sum_p [w_{1N}^p h_p] + b_{1N} \\\\\n",
    "\\sum_p [w_{21}^p h_p] + b_{21} & \\sum_p [w_{22}^p h_p] + b_{22} & \\dots & \\sum_p [w_{2N}^p h_p] + b_{2N} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_p [w_{M1}^p h_p] + b_{M1} & \\sum_p [w_{M2}^p h_p] + b_{M2} & \\dots & \\sum_p [w_{MN}^p h_p] + b_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{W}\\mathbf{h}$ represents [broadcasting](https://numpy.org/devdocs/user/theory.broadcasting.html) $\\mathbf{h}$ over $\\mathbf{W}$, such that:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\sum_p [w_{ij}^p h_p] + b_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}{\\sum_j \\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})} = \\frac{\\exp(y_{ij})}{\\sum_j y_{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(y_{ij})}{\\sum_j y_{ij}}\n",
    "$$\n",
    "\n",
    "Is equivalent to evaluating the softmax function over all columns of $\\mathbf{Y}$ on the $i^{th}$ row, then this leads to an efficient way of computing $p(v_{ij} = 1|\\mathbf{h})$ for all $i$ and $j$.\n",
    "\n",
    "An example of this is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visible = 4\n",
    "num_categories = 5\n",
    "num_hidden = 3\n",
    "\n",
    "# W has a size of 4 x 5 x 3, as shown above\n",
    "W = torch.randn(num_visible,num_categories,num_hidden)\n",
    "# h has a size of 3, as shown above\n",
    "h = torch.randn(num_hidden)\n",
    "# B has a size of 4 x 5, as shown above\n",
    "B = torch.randn(num_visible,num_categories)\n",
    "\n",
    "# compute Wh, which involves broadcasting h over W. Note that broadcasting h over W\n",
    "# returns the same shape as W, as shown below\n",
    "Wh = torch.multiply(W,h)\n",
    "Wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063, -2.7987,  2.4970,  2.7969, -0.4388],\n",
       "        [-0.9202,  1.0963,  1.0452, -0.2078,  1.0426],\n",
       "        [ 0.0595,  2.1118, -1.3849, -0.0870, -0.7365],\n",
       "        [ 1.0304, -0.0254, -0.8133,  0.0376, -3.9434]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Y = \\sum_p [Wh] + B, which involves an element-wise summation and a matrix\n",
    "# summation of B\n",
    "Y = torch.sum(Wh,dim = 2) + B\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0112, 0.0021, 0.4107, 0.5543, 0.0218],\n",
       "        [0.0403, 0.3028, 0.2877, 0.0822, 0.2870],\n",
       "        [0.0967, 0.7532, 0.0228, 0.0836, 0.0436],\n",
       "        [0.5309, 0.1847, 0.0840, 0.1967, 0.0037]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the softmax of each element of Y along its columns to compute p(v_ij = 1|h)\n",
    "p_V_given_h = torch.nn.functional.softmax(Y,dim = 1)\n",
    "p_V_given_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that each row is a categorical probability distribution by checking that\n",
    "# each row sums to 1\n",
    "p_V_given_h.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample a matrix V given the categorical probability distributions\n",
    "V = torch.distributions.one_hot_categorical.OneHotCategorical(probs = p_V_given_h).sample().to(torch.int)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to equation (2) in section 2.1 in [1], the conditional probability that the $p^{th}$ row of $\\mathbf{h}$ is equal to 1 given that the matrix $\\mathbf{V}$ is observed is also modelled using the logistic model:\n",
    "\n",
    "$$\n",
    "p(h_p|\\mathbf{V}) = \\sigma\\left(\\sum_{i,j} \\left[\\mathbf{W}^{[p]} \\odot \\mathbf{V}\\right] + c_p\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\sigma(\\cdot)$ is the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
    "* $\\mathbf{W}^{[p]} \\odot \\mathbf{V}$ is the element-wise product of the $p^{th}$ $M \\times N$ matrix of $\\mathbf{W}$ and the matrix of one-hot encoded visible units $\\mathbf{V}$. Since $\\mathbf{W}$ has a size of $M \\times N \\times K$, where $K$ is the number of hidden units, then there are $K$ possible $M \\times N$ weight matrices, denoted as $\\mathbf{W}^{[1]},\\mathbf{W}^{[2]},...,\\mathbf{W}^{[K]}$. The concatenation of these weight matrices in the third dimension results in the original $\\mathbf{W}$ matrix.\n",
    "* $\\sum_{i,j} \\left[\\mathbf{W}^{[p]} \\odot \\mathbf{V}\\right]$ is the summation over all $M$ rows and $N$ columns of the resulting $M \\times N$ matrix from the element-wise product of $\\mathbf{W}^{[p]}$ and $\\mathbf{V}$, which results in a scalar.\n",
    "* $c_p$ is the scalar bias associated with the $p^{th}$ row of $\\mathbf{h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above conditional probability distribution is also equivalent to:\n",
    "\n",
    "$$\n",
    "p(h_p|\\mathbf{V}) = \\sigma\\left(\\sum_{i,j} \\left[w_{ij}^{[p]} v_{ij}\\right] + c_p\\right)\n",
    "$$\n",
    "\n",
    "Which is how this conditional probability distribution is expressed in equation (2) in section 2.1 in [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each element of $\\mathbf{h}$ is associated with a weight matrix $\\mathbf{W}_{[p]}$ and a scalar bias $c_p$, then it is helpful to express these weights and biases in matrix and vector form. The weight matrices $\\mathbf{W}^{[1]},\\mathbf{W}^{[2]},...,\\mathbf{W}^{[K]}$ can be concatenated together in the third dimension to form the original 3-dimensional weight matrix $\\mathbf{W}$. The scalar biases $c_1,c_2,...,c_K$ can be stored in a vector $\\mathbf{c}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now let:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} \\odot \\mathbf{V}\n",
    "$$\n",
    "\n",
    "Be a $M \\times N \\times K$ 3-dimensional matrix that represents broadcasting element-wise multiplication of $\\mathbf{V}$ over $\\mathbf{W}$, such that $\\mathbf{V}$ is multiplied individually and element-wise with each $M \\times N$ weight matrix in $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let:\n",
    "\n",
    "$$\n",
    "\\mathbf{q} = \\sum_{i,j} [\\mathbf{W} \\odot \\mathbf{V}] + \\mathbf{c}\n",
    "$$\n",
    "\n",
    "Be a $K$-dimensional vector such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{q} = \\begin{bmatrix} \\sum_{i,j} \\left[\\mathbf{W}^{[1]} \\odot \\mathbf{V}\\right] + c_1 & \\sum_{i,j} \\left[\\mathbf{W}^{[2]} \\odot \\mathbf{V}\\right] + c_2 & \\dots & \\sum_{i,j} \\left[\\mathbf{W}^{[K]} \\odot \\mathbf{V}\\right] + c_K \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $\\sum_{i,j} \\left[\\mathbf{W}^{[p]} \\odot \\mathbf{V}\\right]$ represents the summation over all rows and columns of the $p^{th}$ $M \\times N$ matrix $\\mathbf{W}^{[p]} \\odot \\mathbf{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that:\n",
    "\n",
    "$$\n",
    "p(h_p|\\mathbf{V}) = \\sigma\\left(q_p\\right)\n",
    "$$\n",
    "\n",
    "This derivation provides an efficient way of computing $p(h_p|\\mathbf{V})$ for all $p$. An example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visible = 4\n",
    "num_categories = 5\n",
    "num_hidden = 3\n",
    "\n",
    "# W has a size of 4 x 5 x 3, as shown above\n",
    "W = torch.randn(num_visible,num_categories,num_hidden)\n",
    "# V has a size of 4 x 5, as shown above\n",
    "V = torch.randn(num_visible,num_categories)\n",
    "# c has a size of 3\n",
    "c = torch.randn(num_hidden)\n",
    "\n",
    "# compute W \\odot V, which involves broadcasting V over W. Note that broadcasting V over\n",
    "# W returns the same shape as W, as shown below. ALso, the permute() method is needed so\n",
    "# that broadcasting will occur. Since W has a shape of 4 x 5 x 3 and V has a shape of\n",
    "# 4 x 5, then W needs to be reshaped to 3 x 4 x 5 such that the V matrix is\n",
    "# individually multiplied by the the three 4 x 5 matrices in W\n",
    "WV = torch.multiply(W.permute(2,0,1),V)\n",
    "WV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.9808, -0.0680, -1.2550])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute q = \\sum_{i,j} W \\odot V + c, which involves invidually summing all rows and\n",
    "# columns together in each of the three 4 x 5 matrices followed by the addition of the\n",
    "# bias vector c\n",
    "q = torch.sum(WV,dim = (1,2)) + c\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9991, 0.4830, 0.2218])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the invidual probabilities p(h_p = 1|V)\n",
    "p_h_given_V = torch.sigmoid(q)\n",
    "p_h_given_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample a a hidden vector h given the three Bernoulli distributions\n",
    "h = torch.distributions.bernoulli.Bernoulli(probs = p_h_given_V).sample().to(torch.int)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the energy function associated with the categorical-Bernoulli RBM is:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{V},\\mathbf{h}) = - \\sum_{i,j,p} \\left[(\\mathbf{W}\\mathbf{h}) \\odot \\mathbf{V}\\right] - \\sum_{i,j} [\\mathbf{V} \\odot \\mathbf{B}] - \\mathbf{c}^T \\mathbf{h}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{W}\\mathbf{h}$ is a $M \\times N \\times K$ 3-dimensional matrix that represents broadcasting $\\mathbf{h}$ over $\\mathbf{W}$, as discussed above.\n",
    "* $(\\mathbf{W}\\mathbf{h}) \\odot \\mathbf{V}$ is a $M \\times N \\times K$ 3-dimensional matrix that represents the element-wise and individual multiplication of $\\mathbf{V}$ with each $M \\times N$ matrix in $\\mathbf{W}\\mathbf{h}$.\n",
    "* $\\sum_{i,j,p} \\left[(\\mathbf{W}\\mathbf{h}) \\odot \\mathbf{V}\\right]$ is a scalar which is equal to the summation over all three dimensions of the matrix $(\\mathbf{W}\\mathbf{h}) \\odot \\mathbf{V}$.\n",
    "* $\\mathbf{V} \\odot \\mathbf{B}$ is a $M \\times N$ matrix that represents the element-wise product of $\\mathbf{V}$ and $\\mathbf{B}$, as discussed above.\n",
    "* $\\sum_{i,j} [\\mathbf{V} \\odot \\mathbf{B}]$ is a scalar that represents the summation over all rows and columns of the $M \\times N$ matrix $\\mathbf{V} \\odot \\mathbf{B}$.\n",
    "* $\\mathbf{c}^T \\mathbf{h}$ is a scalar that represents the inner product between $\\mathbf{c}$ and $\\mathbf{h}$.\n",
    "\n",
    "Here is how the energy function is computed in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1643)\n",
      "tensor(5.9659)\n",
      "tensor(-0.7572)\n",
      "tensor(-6.3729)\n"
     ]
    }
   ],
   "source": [
    "num_visible = 4\n",
    "num_categories = 5\n",
    "num_hidden = 3\n",
    "\n",
    "V = torch.randn(num_visible,num_categories)\n",
    "h = torch.randn(num_hidden)\n",
    "W = torch.randn(num_visible,num_categories,num_hidden)\n",
    "b = torch.randn(num_visible,num_categories)\n",
    "c = torch.randn(num_hidden)\n",
    "\n",
    "# first term of energy function\n",
    "\n",
    "Wh = torch.multiply(W,h)\n",
    "WhV = torch.multiply(Wh.permute(2,0,1),V)\n",
    "first_term = torch.sum(WhV)\n",
    "print(first_term)\n",
    "\n",
    "# second term of energy function\n",
    "\n",
    "VB = torch.multiply(V,B)\n",
    "second_term = torch.sum(VB)\n",
    "print(second_term)\n",
    "\n",
    "# third term of energy function\n",
    "\n",
    "third_term = torch.dot(c,h)\n",
    "print(third_term)\n",
    "\n",
    "# energy\n",
    "\n",
    "energy = -first_term - second_term - third_term\n",
    "print(energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
