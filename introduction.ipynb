{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines and Deep Belief Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Restricted Boltzmann Machine (RBM) is a probabilistic graphical model that consists of visible random variables (or units) $\\mathbf{v}$ and hidden random variables (or units) $\\mathbf{h}$, where:\n",
    "\n",
    "$$\\mathbf{v}=\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_N \\end{bmatrix}$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\mathbf{h}=\\begin{bmatrix} h_1 \\\\ h_2 \\\\ \\vdots \\\\ h_M \\end{bmatrix}$$\n",
    "\n",
    "See [section 16.5](https://www.deeplearningbook.org/contents/graphical_models.html) in the book titled \"Deep Learning\" by Ian Goodfellow et al. for why the hidden vector $\\mathbf{h}$ is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution represented by all [Boltzmann Machines](https://en.wikipedia.org/wiki/Boltzmann_machine) (not just an RBM) is:\n",
    "\n",
    "$$p(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$\n",
    "\n",
    "Where $\\mathbf{W}$ is a parameter matrix, $\\mathbf{a}$ and $\\mathbf{b}$ are parameter vectors, and:\n",
    "\n",
    "$$E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = -\\mathbf{a}^T\\mathbf{v} - \\mathbf{b}^T\\mathbf{h} - \\mathbf{v}^T\\mathbf{W}\\mathbf{h}$$\n",
    "\n",
    "$$Z = \\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E$ is know as the **energy function** and $Z$ is known as the **partition function**. However, to make parameter estimation (learning) and inference tractable, a **Restricted** Boltzmann Machine introduces the following assumptions:\n",
    "\n",
    "$$ p(\\mathbf{v}|\\mathbf{h}) = \\prod_{i=1}^N p(v_i|\\mathbf{h})$$\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\prod_{j=1}^M p(h_i|\\mathbf{v})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, it is assumed that the individual visible random variables (units) are conditionally independent given all the hidden units $\\mathbf{h}$. Also, it is assumed that the individual hidden units are conditionally independent given all the visible units $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable to estimate the marginal probability distribution $p(\\mathbf{v})$ of all the visible units $\\mathbf{v}$, as this reveals relationships between the visible units. This marginal probability distribution can be obtained by marginalizing the joint distribution $p(\\mathbf{v},\\mathbf{h})$ over all the hidden units $\\mathbf{h}$:\n",
    "\n",
    "$$p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marginal probability distribution can therefore be estimated using maximum likelihood estimation. Assuming that the visible units $\\mathbf{v}$ have been observed, then the equation above also represents the likelihood function. The log-likelihood function is therefore:\n",
    "\n",
    "$$ln(p(\\mathbf{v};\\mathbf{\\theta})) = ln\\left(\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right) - ln\\left(\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\theta = \\begin{bmatrix} \\mathbf{W} \\\\ \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix}$$\n",
    "\n",
    "For brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the log-likelihood function is not in closed form, which means that it is difficult to compute analytically, then gradient ascent will be used to maximize it. The gradient of the log-likelihood function is:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\frac{1}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\frac{p(\\mathbf{v},\\mathbf{h})}{p(\\mathbf{v})} = \\frac{\\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since:\n",
    "\n",
    "$$\\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{Z} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = p(\\mathbf{v},\\mathbf{h})$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{v},\\mathbf{h})\\right] \\\\\n",
    "&= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both of these are expectations, they can be approximated using [Monte Carlo integration](http://people.duke.edu/~ccc14/sta-663-2019/notebook/S14D_Monte_Carlo_Integration.html#Intuition-behind-Monte-Carlo-integration):\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} \\approx -\\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] +\n",
    "\\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term can be computed because it is easy to sample from $p(\\mathbf{h}|\\mathbf{v})$. However, it is difficult to sample from $p(\\mathbf{v},\\mathbf{h})$ directly, but since it is easy to sample from $p(\\mathbf{v}|\\mathbf{h})$, then Gibbs sampling is used to sample from both $p(\\mathbf{h}|\\mathbf{v})$ and $p(\\mathbf{v}|\\mathbf{h})$ to approximate a sample from $p(\\mathbf{v},\\mathbf{h})$. Also, notice that:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx \\frac{\\partial}{\\partial \\mathbf{\\theta}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta}) \\right] \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this means that the gradient of the log-likelihood function can be approximated by the gradient of the difference of two sample means. Once the gradient of the log-likelihood function is estimated for a particular set of observed visible units $\\mathbf{v}$, then the parameters $\\mathbf{W},\\mathbf{a},$ and $\\mathbf{b}$ can be updated using the following gradient ascent update rules:\n",
    "\n",
    "$$\\mathbf{W}_{t+1} = \\mathbf{W}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "$$\\mathbf{a}_{t+1} = \\mathbf{a}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}}$$\n",
    "\n",
    "$$\\mathbf{b}_{t+1} = \\mathbf{b}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}}$$\n",
    "\n",
    "Where $\\epsilon$ is the learning rate and:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}} \\approx \\frac{\\partial}{\\partial \\mathbf{W}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}} \\approx \\frac{\\partial}{\\partial \\mathbf{a}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}} \\approx \\frac{\\partial}{\\partial \\mathbf{b}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of an RBM is inspired by the paper titled [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) by Ruslan Salakhutdinov et al., which will from now on be referenced as [1].\n",
    "\n",
    "More precisely, one of the RBM architectures that the authors implemented involved categorical (or softmax) visible units and Bernoulli (or binary) hidden units. Also, in the paper, the authors use data from the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) competition that consists of approximately 100 million user ratings. 480,000 users gave approximately 18,000 movies a rating from 1 to 5. Therefore, the main objective of this implementation is to use these user ratings as the categorical visible units with Bernoulli hidden units and estimate the corresponding parameters using maximum likelihood estimation. However, to keep the implementation simple, only 50 movies per user will be considered.\n",
    "\n",
    "This RBM will be implemented using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before considering 50 movies per user, to make the categorical-Bernoulli RBM easier to understand, suppose instead that there are only 4 movie ratings per user and that the relationships between these movie ratings will be encoded in only 3 hidden Bernoulli units. Note that since user ratings range from 1 to 5, they will need to be expressed in one-hot encoding format (see [here](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that given that there are 4 movie ratings per user ranging from 1 to 5, then the visible units will be expressed in a $4 \\times 5$ matrix, denoted $\\mathbf{V}$, where the $i^{th}$ row corresponds to the $i^{th}$ movie per user, and the $j^{th}$ column of the $i^{th}$ row corresponds to whether that user gave a rating of $j$ to the $i^{th}$ movie. For example, $\\mathbf{V}$ could be:\n",
    "\n",
    "$$\n",
    "\\mathbf{V} =\n",
    "\\begin{bmatrix} \n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that this user gave the first movie a rating of 2, the second movie a rating of 3, the third movie a rating of 1, and the fourth movie a rating of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, given $M$ movie ratings per user, with each rating ranging from 1 to $N$, then $\\mathbf{V}$ is a $M \\times N$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{V} = \n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} & \\dots & v_{1N} \\\\\n",
    "v_{21} & v_{22} & v_{23} & \\dots & v_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{M1} & v_{M2} & v_{M3} & \\dots & v_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, one example of a $\\mathbf{V}$ matrix is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_visible = 4\n",
    "num_categories = 5\n",
    "V = torch.randn(num_visible,num_categories)\n",
    "# make sure each row is a categorical distribution\n",
    "V = torch.nn.functional.softmax(V,dim = 1)\n",
    "# sample\n",
    "V = torch.distributions.one_hot_categorical.OneHotCategorical(probs = V).sample().to(torch.int)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the hidden units are Bernoulli (binary), then it is sufficient to model the hidden units as a $K \\times 1$ column vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "\\vdots \\\\\n",
    "h_K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For example, in the case of 3 hidden units, $\\mathbf{h}$ could be:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, one example of $\\mathbf{h}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden = 3\n",
    "h = torch.randn(num_hidden)\n",
    "# make sure each element is a Bernoulli distribution\n",
    "h = torch.sigmoid(h)\n",
    "# sample\n",
    "h = torch.distributions.bernoulli.Bernoulli(probs = h).sample().to(torch.int)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to equation (1) in section 2.1 from [1], the conditional probability that the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$ is 1 given that the hidden vector $\\mathbf{h}$ is observed is modelled as a logistic model:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\mathbf{w}_{ij}^T \\mathbf{h} + b_{ij})}{\\sum_j \\exp(\\mathbf{w}_{ij}^T \\mathbf{h} + b_{ij})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $\\mathbf{w}_{ij}$ is the weight vector associated with the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$.\n",
    "* $b_{ij}$ is the scalar bias associated with the $i^{th}$ row and $j^{th}$ column of $\\mathbf{V}$.\n",
    "\n",
    "In other words, each row of $\\mathbf{V}$ is sampled from a categorical distribution then converted to one-hot encoding.\n",
    "\n",
    "Note that since an inner product is equivalent to an element-wise multiplication followed by a summation, and if:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{ij} =\n",
    "\\begin{bmatrix}\n",
    "w_{ij}^1 \\\\\n",
    "w_{ij}^2 \\\\\n",
    "w_{ij}^3 \\\\\n",
    "\\vdots \\\\\n",
    "w_{ij}^K\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the above conditional probability distribution can also be expressed as:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}{\\sum_j \\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}\n",
    "$$\n",
    "\n",
    "Which is how this conditional probability distribution is expressed in the equation (1) in section 2.1 in [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each element of $\\mathbf{V}$ is associated with a weight vector $\\mathbf{w}_{ij}$ and bias $b_{ij}$, then it is helpful to express these weights and biases as matrices. The weight vectors $\\mathbf{w}_{ij}$ can be stored in a 3-dimensional matrix, denoted $\\mathbf{W}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}_{11} & \\mathbf{w}_{12} & \\mathbf{w}_{13} & \\dots & \\mathbf{w}_{1N} \\\\\n",
    "\\mathbf{w}_{21} & \\mathbf{w}_{22} & \\mathbf{w}_{23} & \\dots & \\mathbf{w}_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{w}_{M1} & \\mathbf{w}_{M2} & \\mathbf{w}_{M3} & \\dots & \\mathbf{w}_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This means that $\\mathbf{W}$ has the dimensions $M \\times N \\times K$, where $K$ is the dimensionality of the hidden vector $\\mathbf{h}$. In the case of 4 movie ratings per user, 5 possible ratings, and 3 hidden units, then $\\mathbf{W}$ has a size of $4 \\times 5 \\times 3$, and each element is a vector of size $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the scalar biases can be stored in a 2-dimensional matrix, denoted $\\mathbf{B}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & b_{13} & \\dots & b_{1N} \\\\\n",
    "b_{21} & b_{22} & b_{23} & \\dots & b_{2N} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{M1} & b_{M2} & b_{M3} & \\dots & b_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the case of 4 movie ratings per user and 5 possible ratings, then $\\mathbf{B}$ has a size of $4 \\times 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\sum_p [\\mathbf{W}\\mathbf{h}] + \\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "\\sum_p [w_{11}^p h_p] + b_{11} & \\sum_p [w_{12}^p h_p] + b_{12} & \\dots & \\sum_p [w_{1N}^p h_p] + b_{1N} \\\\\n",
    "\\sum_p [w_{21}^p h_p] + b_{21} & \\sum_p [w_{22}^p h_p] + b_{22} & \\dots & \\sum_p [w_{2N}^p h_p] + b_{2N} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_p [w_{M1}^p h_p] + b_{M1} & \\sum_p [w_{M2}^p h_p] + b_{M2} & \\dots & \\sum_p [w_{MN}^p h_p] + b_{MN} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{W}\\mathbf{h}$ represents [broadcasting](https://numpy.org/devdocs/user/theory.broadcasting.html) $\\mathbf{h}$ over $\\mathbf{W}$, such that:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\sum_p [w_{ij}^p h_p] + b_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})}{\\sum_j \\exp(\\sum_p [w_{ij}^p h_p] + b_{ij})} = \\frac{\\exp(y_{ij})}{\\sum_j y_{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "\n",
    "$$\n",
    "p(v_{ij} = 1|\\mathbf{h}) = \\frac{\\exp(y_{ij})}{\\sum_j y_{ij}}\n",
    "$$\n",
    "\n",
    "Is equivalent to evaluating the softmax function over all columns of $\\mathbf{Y}$ on the $i^{th}$ row, then this leads to an efficient way of computing $p(v_{ij} = 1|\\mathbf{h})$ for all $i$ and $j$.\n",
    "\n",
    "An example of this is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visible = 4\n",
    "num_categories = 5\n",
    "num_hidden = 3\n",
    "\n",
    "# W has a size of 4 x 5 x 3, as shown above\n",
    "W = torch.randn(num_visible,num_categories,num_hidden)\n",
    "# h has a size of 3, as shown above\n",
    "h = torch.randn(num_hidden)\n",
    "# B has a size of 4 x 5, as shown above\n",
    "B = torch.randn(num_visible,num_categories)\n",
    "\n",
    "# compute Wh, which involves broadcasting h over W. Note that broadcasting h over W\n",
    "# returns the same shape as W, as shown below\n",
    "q = torch.multiply(W,h)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4834,  0.1508,  1.4496, -1.5150, -0.6009],\n",
       "        [-0.3032,  0.3244,  0.4761,  0.0717, -0.6289],\n",
       "        [-0.5083,  0.5858,  0.4626, -1.8073,  0.8710],\n",
       "        [ 3.0285,  1.9990,  0.2142,  0.3210, -2.7396]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute \\sum_p [Wh], which involves the element-wise summation\n",
    "q = torch.sum(q,dim = 2)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0459, -1.9429,  1.9257, -1.5054, -0.9010],\n",
       "        [ 1.4032,  1.3421,  0.9941,  1.2638,  1.6400],\n",
       "        [-0.7182,  2.3766,  1.0663, -2.1829,  0.7892],\n",
       "        [ 2.9637,  2.9398,  2.3773,  0.4464, -1.2851]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Y = \\sum_p [Wh] + B\n",
    "Y = q + B\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1206, 0.0165, 0.7905, 0.0256, 0.0468],\n",
       "        [0.2109, 0.1984, 0.1401, 0.1834, 0.2672],\n",
       "        [0.0296, 0.6536, 0.1763, 0.0068, 0.1336],\n",
       "        [0.3806, 0.3716, 0.2117, 0.0307, 0.0054]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the softmax of each element of Y along its columns\n",
    "p_v_given_h = torch.nn.functional.softmax(Y,dim = 1)\n",
    "p_v_given_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that each row is a categorical probability distribution by checking that\n",
    "# each row sums to 1\n",
    "p_v_given_h.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample a matrix V given the categorical probability distributions\n",
    "V = torch.distributions.one_hot_categorical.OneHotCategorical(probs = p_v_given_h).sample().to(torch.int)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the parameters $\\mathbf{W},\\mathbf{a},$ and $\\mathbf{b}$ need to be initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visible = 50\n",
    "num_hidden = 25\n",
    "W = torch.randn(num_hidden,num_visible)\n",
    "a = torch.randn(num_visible,1)\n",
    "b = torch.randn(num_hidden,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7279,  0.0470,  0.2512,  ..., -0.7241, -0.0427, -1.6977],\n",
       "        [-0.4283,  0.4730,  0.3296,  ...,  0.1101,  0.4922, -0.4110],\n",
       "        [-1.7327, -1.9367, -1.1274,  ...,  0.8164, -0.5042,  1.1784],\n",
       "        ...,\n",
       "        [ 1.0491, -1.0257, -0.9975,  ..., -0.5079, -2.1602,  0.5719],\n",
       "        [ 0.9663, -0.3578, -0.4960,  ...,  0.3218,  0.9213,  0.6263],\n",
       "        [ 0.7383,  0.5950,  0.8808,  ...,  0.0477,  0.9073, -1.1476]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5862],\n",
       "        [ 1.5568],\n",
       "        [-0.2033],\n",
       "        [ 0.0202],\n",
       "        [-0.5568],\n",
       "        [ 2.2841],\n",
       "        [ 0.1458],\n",
       "        [ 0.9263],\n",
       "        [-0.2177],\n",
       "        [ 2.0682],\n",
       "        [ 0.0493],\n",
       "        [ 0.1205],\n",
       "        [-0.3217],\n",
       "        [-1.7253],\n",
       "        [ 0.5958],\n",
       "        [-0.3676],\n",
       "        [-1.6230],\n",
       "        [-0.2149],\n",
       "        [-0.2773],\n",
       "        [-1.1704],\n",
       "        [ 0.6007],\n",
       "        [-2.5750],\n",
       "        [-0.4187],\n",
       "        [-0.7106],\n",
       "        [-0.7953],\n",
       "        [ 0.6599],\n",
       "        [-0.4423],\n",
       "        [ 2.0682],\n",
       "        [-1.0603],\n",
       "        [ 0.3971],\n",
       "        [ 1.3892],\n",
       "        [-0.9456],\n",
       "        [-0.4934],\n",
       "        [ 0.7437],\n",
       "        [-0.0402],\n",
       "        [-0.9114],\n",
       "        [ 0.0707],\n",
       "        [ 1.1751],\n",
       "        [ 1.9328],\n",
       "        [-0.5940],\n",
       "        [ 1.5585],\n",
       "        [-1.9806],\n",
       "        [-0.8305],\n",
       "        [-0.1005],\n",
       "        [-1.4645],\n",
       "        [ 0.0216],\n",
       "        [ 0.7611],\n",
       "        [-1.1748],\n",
       "        [ 0.6165],\n",
       "        [-0.1728]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8113],\n",
       "        [ 0.5568],\n",
       "        [-0.4792],\n",
       "        [ 0.4992],\n",
       "        [-1.2462],\n",
       "        [-0.5796],\n",
       "        [ 0.2035],\n",
       "        [ 0.2655],\n",
       "        [ 1.4925],\n",
       "        [-1.4995],\n",
       "        [-0.3472],\n",
       "        [-2.0273],\n",
       "        [ 1.2186],\n",
       "        [-1.4558],\n",
       "        [-0.4875],\n",
       "        [ 0.0735],\n",
       "        [ 0.2867],\n",
       "        [ 0.0129],\n",
       "        [-0.5585],\n",
       "        [ 0.2561],\n",
       "        [ 0.6355],\n",
       "        [ 0.6224],\n",
       "        [-1.5228],\n",
       "        [ 1.6203],\n",
       "        [-0.7766]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
