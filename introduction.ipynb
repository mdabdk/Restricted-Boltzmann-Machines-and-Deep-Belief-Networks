{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines and Deep Belief Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Restricted Boltzmann Machine (RBM) is a probabilistic graphical model that consists of visible random variables (or units) $\\mathbf{v}$ and hidden random variables (or units) $\\mathbf{h}$, where:\n",
    "\n",
    "$$\\mathbf{v}=\\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_N \\end{bmatrix}$$\n",
    "\n",
    "And:\n",
    "\n",
    "$$\\mathbf{h}=\\begin{bmatrix} h_1 \\\\ h_2 \\\\ \\vdots \\\\ h_M \\end{bmatrix}$$\n",
    "\n",
    "See [section 16.5](https://www.deeplearningbook.org/contents/graphical_models.html) in the book titled \"Deep Learning\" by Ian Goodfellow et al. for why the hidden vector $\\mathbf{h}$ is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution represented by all [Boltzmann Machines](https://en.wikipedia.org/wiki/Boltzmann_machine) (not just an RBM) is:\n",
    "\n",
    "$$p(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$\n",
    "\n",
    "Where $\\mathbf{W}$ is a parameter matrix, $\\mathbf{a}$ and $\\mathbf{b}$ are parameter vectors, and:\n",
    "\n",
    "$$E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = -\\mathbf{a}^T\\mathbf{v} - \\mathbf{b}^T\\mathbf{h} - \\mathbf{v}^T\\mathbf{W}\\mathbf{h}$$\n",
    "\n",
    "$$Z = \\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E$ is know as the **energy function** and $Z$ is known as the **partition function**. However, to make parameter estimation (learning) and inference tractable, a **Restricted** Boltzmann Machine introduces the following assumptions:\n",
    "\n",
    "$$ p(\\mathbf{v}|\\mathbf{h}) = \\prod_{i=1}^N p(v_i|\\mathbf{h})$$\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\prod_{j=1}^M p(h_i|\\mathbf{v})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, it is assumed that the individual visible random variables (units) are conditionally independent given all the hidden units $\\mathbf{h}$. Also, it is assumed that the individual hidden units are conditionally independent given all the visible units $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is desirable to estimate the marginal probability distribution $p(\\mathbf{v})$ of all the visible units $\\mathbf{v}$, as this reveals relationships between the visible units. This marginal probability distribution can be obtained by marginalizing the joint distribution $p(\\mathbf{v},\\mathbf{h})$ over all the hidden units $\\mathbf{h}$:\n",
    "\n",
    "$$p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{W},\\mathbf{a},\\mathbf{b})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marginal probability distribution can therefore be estimated using maximum likelihood estimation. Assuming that the visible units $\\mathbf{v}$ have been observed, then the equation above also represents the likelihood function. The log-likelihood function is therefore:\n",
    "\n",
    "$$ln(p(\\mathbf{v};\\mathbf{\\theta})) = ln\\left(\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right) - ln\\left(\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\theta = \\begin{bmatrix} \\mathbf{W} \\\\ \\mathbf{a} \\\\ \\mathbf{b} \\end{bmatrix}$$\n",
    "\n",
    "For brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the log-likelihood function is not in closed form, which means that it is difficult to compute analytically, then gradient ascent will be used to maximize it. The gradient of the log-likelihood function is:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\frac{1}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "\n",
    "$$p(\\mathbf{h}|\\mathbf{v}) = \\frac{p(\\mathbf{v},\\mathbf{h})}{p(\\mathbf{v})} = \\frac{\\frac{1}{Z} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\frac{1}{Z} \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} = -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\frac{1}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} \\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since:\n",
    "\n",
    "$$\\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{Z} = \\frac{e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}}{\\sum_{\\mathbf{v},\\mathbf{h}} e^{-E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}} = p(\\mathbf{v},\\mathbf{h})$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\sum_{\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{h}|\\mathbf{v}) \\right] +\n",
    "\\sum_{\\mathbf{v},\\mathbf{h}} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\cdot p(\\mathbf{v},\\mathbf{h})\\right] \\\\\n",
    "&= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both of these are expectations, they can be approximated using [Monte Carlo integration](http://people.duke.edu/~ccc14/sta-663-2019/notebook/S14D_Monte_Carlo_Integration.html#Intuition-behind-Monte-Carlo-integration):\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} \\approx -\\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] +\n",
    "\\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term can be computed because it is easy to sample from $p(\\mathbf{h}|\\mathbf{v})$. However, it is difficult to sample from $p(\\mathbf{v},\\mathbf{h})$ directly, but since it is easy to sample from $p(\\mathbf{v}|\\mathbf{h})$, then Gibbs sampling is used to sample from both $p(\\mathbf{h}|\\mathbf{v})$ and $p(\\mathbf{v}|\\mathbf{h})$ to approximate a sample from $p(\\mathbf{v},\\mathbf{h})$. Also, notice that:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{\\theta}))}{\\partial \\mathbf{\\theta}} &= -\\mathbb{E}_{p(\\mathbf{h}|\\mathbf{v})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\mathbb{E}_{p(\\mathbf{v},\\mathbf{h})}\\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h};\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[\\frac{\\partial E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] + \\frac{1}{M} \\sum_{j=1}^{M} \\left[\\frac{\\partial E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta})}{\\partial \\mathbf{\\theta}} \\right] \\\\\n",
    "&\\approx \\frac{\\partial}{\\partial \\mathbf{\\theta}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{\\theta}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{\\theta}) \\right] \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this means that the gradient of the log-likelihood function can be approximated by the gradient of the difference of two sample means. Once the gradient of the log-likelihood function is estimated for a particular set of observed visible units $\\mathbf{v}$, then the parameters $\\mathbf{W},\\mathbf{a},$ and $\\mathbf{b}$ can be updated using the following gradient ascent update rules:\n",
    "\n",
    "$$\\mathbf{W}_{t+1} = \\mathbf{W}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "$$\\mathbf{a}_{t+1} = \\mathbf{a}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}}$$\n",
    "\n",
    "$$\\mathbf{b}_{t+1} = \\mathbf{b}_{t} + \\epsilon \\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}}$$\n",
    "\n",
    "Where $\\epsilon$ is the learning rate and:\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{W}} \\approx \\frac{\\partial}{\\partial \\mathbf{W}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{a}} \\approx \\frac{\\partial}{\\partial \\mathbf{a}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$\n",
    "\n",
    "$$\\frac{\\partial ln(p(\\mathbf{v};\\mathbf{W},\\mathbf{a},\\mathbf{b}))}{\\partial \\mathbf{b}} \\approx \\frac{\\partial}{\\partial \\mathbf{b}} \\left(\\frac{1}{M} \\sum_{j=1}^{M} \\left[E(\\mathbf{v}_j,\\mathbf{h}_j;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] - \\frac{1}{N} \\sum_{i = 1}^{N} \\left[E(\\mathbf{v},\\mathbf{h}_i;\\mathbf{W},\\mathbf{a},\\mathbf{b}) \\right] \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of an RBM is inspired by the paper titled [Restricted Boltzmann Machines for Collaborative Filtering](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf) by Ruslan Salakhutdinov et al. More precisely, one of the RBM architectures that the authors implemented involved categorical (or softmax) visible units and Bernoulli (or binary) hidden units. Also, in the paper, the authors use data from the [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize) competition that consists of approximately 100 million user ratings. 480,000 users gave approximately 18,000 movies a rating from 1 to 5. Therefore, the main objective of this implementation is to use these user ratings as the categorical visible units with Bernoulli hidden units and estimate the corresponding parameters using maximum likelihood estimation. However, to keep the implementation simple, only 50 movies will be considered.\n",
    "\n",
    "This RBM will be implemented using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 50 movies, this means that there are 50 visible categorical units. Suppose that there are 25 hidden Bernoulli units. Note that since user ratings range from 1 to 5, they will need to be expressed in one-hot encoding format (see [here](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for an explanation). Therefore, suppose that 50 movie ratings by a single user are obtained in one-hot encoding format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1721, 0.2829, 0.0214, 0.0192, 0.5043],\n",
       "        [0.0867, 0.4595, 0.1345, 0.1894, 0.1299],\n",
       "        [0.2326, 0.3708, 0.1514, 0.1246, 0.1206],\n",
       "        [0.2243, 0.1991, 0.2731, 0.2845, 0.0189],\n",
       "        [0.0191, 0.1250, 0.5236, 0.2545, 0.0778],\n",
       "        [0.4259, 0.1737, 0.1083, 0.1265, 0.1656],\n",
       "        [0.5671, 0.0830, 0.2285, 0.0806, 0.0408],\n",
       "        [0.1066, 0.0758, 0.1777, 0.3223, 0.3176],\n",
       "        [0.3902, 0.0944, 0.1910, 0.2639, 0.0605],\n",
       "        [0.1557, 0.0934, 0.3536, 0.1880, 0.2092],\n",
       "        [0.0094, 0.0158, 0.0719, 0.2030, 0.6999],\n",
       "        [0.0303, 0.0359, 0.2922, 0.5208, 0.1208],\n",
       "        [0.5213, 0.0612, 0.1281, 0.0755, 0.2138],\n",
       "        [0.2511, 0.0487, 0.3005, 0.3665, 0.0332],\n",
       "        [0.0609, 0.0891, 0.0405, 0.1755, 0.6340],\n",
       "        [0.1500, 0.2950, 0.1157, 0.1814, 0.2579],\n",
       "        [0.0960, 0.2146, 0.4050, 0.0175, 0.2669],\n",
       "        [0.1827, 0.4937, 0.2408, 0.0797, 0.0031],\n",
       "        [0.2269, 0.0329, 0.0906, 0.3974, 0.2521],\n",
       "        [0.1084, 0.0185, 0.5870, 0.0602, 0.2258],\n",
       "        [0.0179, 0.4199, 0.1774, 0.1307, 0.2541],\n",
       "        [0.0655, 0.2317, 0.1477, 0.4364, 0.1187],\n",
       "        [0.1628, 0.1833, 0.0675, 0.3973, 0.1890],\n",
       "        [0.6817, 0.0216, 0.0621, 0.0532, 0.1813],\n",
       "        [0.1969, 0.1502, 0.3963, 0.2121, 0.0445],\n",
       "        [0.5167, 0.1668, 0.0373, 0.1091, 0.1700],\n",
       "        [0.0133, 0.0411, 0.1099, 0.2318, 0.6040],\n",
       "        [0.4117, 0.2994, 0.0666, 0.1465, 0.0759],\n",
       "        [0.0327, 0.0322, 0.0884, 0.2976, 0.5490],\n",
       "        [0.5473, 0.0705, 0.0828, 0.1147, 0.1847],\n",
       "        [0.0248, 0.6947, 0.2101, 0.0447, 0.0257],\n",
       "        [0.1404, 0.1816, 0.1389, 0.2642, 0.2749],\n",
       "        [0.5672, 0.1406, 0.0320, 0.0579, 0.2023],\n",
       "        [0.1205, 0.3513, 0.0239, 0.0734, 0.4309],\n",
       "        [0.3942, 0.0667, 0.0810, 0.3859, 0.0722],\n",
       "        [0.2262, 0.0601, 0.4413, 0.1676, 0.1047],\n",
       "        [0.0525, 0.4492, 0.1063, 0.2386, 0.1534],\n",
       "        [0.4382, 0.2845, 0.0211, 0.1341, 0.1220],\n",
       "        [0.2028, 0.3671, 0.0468, 0.0671, 0.3162],\n",
       "        [0.1290, 0.0771, 0.5219, 0.1528, 0.1192],\n",
       "        [0.4370, 0.0842, 0.0633, 0.3047, 0.1107],\n",
       "        [0.0471, 0.0829, 0.0859, 0.7385, 0.0455],\n",
       "        [0.4229, 0.1651, 0.2578, 0.0834, 0.0709],\n",
       "        [0.2160, 0.1030, 0.3752, 0.1688, 0.1369],\n",
       "        [0.1123, 0.1313, 0.1139, 0.5275, 0.1150],\n",
       "        [0.1768, 0.1359, 0.4253, 0.1557, 0.1063],\n",
       "        [0.0507, 0.1964, 0.2287, 0.1296, 0.3947],\n",
       "        [0.0167, 0.0261, 0.5376, 0.0771, 0.3425],\n",
       "        [0.7148, 0.0752, 0.0803, 0.0489, 0.0808],\n",
       "        [0.0460, 0.3788, 0.0432, 0.4836, 0.0484]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create a categorical probability distribution for each of the 50 movie ratings. Note\n",
    "# that these probability distributions will be helpful for validating results later on\n",
    "\n",
    "v = torch.randn((50,5))\n",
    "v = torch.nn.functional.softmax(v,dim = 1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that each row adds up to 1\n",
    "\n",
    "v.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 50 one-hot encoded vectors from each categorical distribution\n",
    "\n",
    "v = torch.distributions.one_hot_categorical.OneHotCategorical(probs = v).sample()\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the parameters $\\mathbf{W},\\mathbf{a},$ and $\\mathbf{b}$ need to be initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visible = 50\n",
    "num_hidden = 25\n",
    "W = torch.randn(num_hidden,num_visible)\n",
    "a = torch.randn(num_visible,1)\n",
    "b = torch.randn(num_hidden,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7279,  0.0470,  0.2512,  ..., -0.7241, -0.0427, -1.6977],\n",
       "        [-0.4283,  0.4730,  0.3296,  ...,  0.1101,  0.4922, -0.4110],\n",
       "        [-1.7327, -1.9367, -1.1274,  ...,  0.8164, -0.5042,  1.1784],\n",
       "        ...,\n",
       "        [ 1.0491, -1.0257, -0.9975,  ..., -0.5079, -2.1602,  0.5719],\n",
       "        [ 0.9663, -0.3578, -0.4960,  ...,  0.3218,  0.9213,  0.6263],\n",
       "        [ 0.7383,  0.5950,  0.8808,  ...,  0.0477,  0.9073, -1.1476]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5862],\n",
       "        [ 1.5568],\n",
       "        [-0.2033],\n",
       "        [ 0.0202],\n",
       "        [-0.5568],\n",
       "        [ 2.2841],\n",
       "        [ 0.1458],\n",
       "        [ 0.9263],\n",
       "        [-0.2177],\n",
       "        [ 2.0682],\n",
       "        [ 0.0493],\n",
       "        [ 0.1205],\n",
       "        [-0.3217],\n",
       "        [-1.7253],\n",
       "        [ 0.5958],\n",
       "        [-0.3676],\n",
       "        [-1.6230],\n",
       "        [-0.2149],\n",
       "        [-0.2773],\n",
       "        [-1.1704],\n",
       "        [ 0.6007],\n",
       "        [-2.5750],\n",
       "        [-0.4187],\n",
       "        [-0.7106],\n",
       "        [-0.7953],\n",
       "        [ 0.6599],\n",
       "        [-0.4423],\n",
       "        [ 2.0682],\n",
       "        [-1.0603],\n",
       "        [ 0.3971],\n",
       "        [ 1.3892],\n",
       "        [-0.9456],\n",
       "        [-0.4934],\n",
       "        [ 0.7437],\n",
       "        [-0.0402],\n",
       "        [-0.9114],\n",
       "        [ 0.0707],\n",
       "        [ 1.1751],\n",
       "        [ 1.9328],\n",
       "        [-0.5940],\n",
       "        [ 1.5585],\n",
       "        [-1.9806],\n",
       "        [-0.8305],\n",
       "        [-0.1005],\n",
       "        [-1.4645],\n",
       "        [ 0.0216],\n",
       "        [ 0.7611],\n",
       "        [-1.1748],\n",
       "        [ 0.6165],\n",
       "        [-0.1728]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8113],\n",
       "        [ 0.5568],\n",
       "        [-0.4792],\n",
       "        [ 0.4992],\n",
       "        [-1.2462],\n",
       "        [-0.5796],\n",
       "        [ 0.2035],\n",
       "        [ 0.2655],\n",
       "        [ 1.4925],\n",
       "        [-1.4995],\n",
       "        [-0.3472],\n",
       "        [-2.0273],\n",
       "        [ 1.2186],\n",
       "        [-1.4558],\n",
       "        [-0.4875],\n",
       "        [ 0.0735],\n",
       "        [ 0.2867],\n",
       "        [ 0.0129],\n",
       "        [-0.5585],\n",
       "        [ 0.2561],\n",
       "        [ 0.6355],\n",
       "        [ 0.6224],\n",
       "        [-1.5228],\n",
       "        [ 1.6203],\n",
       "        [-0.7766]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
